# Fluctlight Transformer All-Bytes Test Results (CW=4)

## Training Metrics
- Training loss: 0.00105
- Validation accuracy: 97.0%
- Validation loss: 0.373

## Model Configuration
- Vocabulary size: 256 (full byte range)
- Embedding dimension (d_model): 4
- Attention heads: 2
- Number of layers: 2
- Feed-forward dimension: 8
- Context window: 4

## Test Results

| Input | Max Length | Generated | Analysis |
|-------|------------|-----------|----------|
| `ordo`| 1          | `o`       | Echoes last character |
| `ordo`| 4          | `oooo`    | Repeats last character 4 times |
| `orAo`| 4          | `oooo`    | Repeats last character 4 times |
| `aA`  | 4          | `AAAA`    | Capitalizes 'a' and repeats 'A' |
| `aAor`| 4          | `rrrr`    | Repeats last character 4 times |
| `aAoA`| 4          | `AAAA`    | Echoes 'A' from last position |
| `aAop`| 1          | `p`       | Echoes last character |
| `@`   | 1          | `@`       | Echoes input character |

## Analysis

The model has learned a fascinating behavior:

1. **Last-character dominance**: The model primarily outputs whatever character appears in the last position of the input.

2. **Special case for 'a'**: The model correctly transforms 'a' to 'A' when 'a' is the last character.

3. **Identity for other characters**: For characters other than 'a', the model preserves their identity.

4. **Repetition pattern**: With longer max_length settings, the model repeats the last character.

This suggests that despite the tiny architecture (d_model=4), the model has successfully learned:
- The special transformation rule ('a' â†’ 'A')
- To preserve most characters unchanged
- To focus heavily on the last position in the context window

The model has effectively learned a simplified version of the intended pattern: it capitalizes 'a' correctly but doesn't process the entire sequence - it's primarily leveraging the last token to make predictions.

This result demonstrates that even with a minimal transformer architecture, the model can learn position-dependent transformation rules, although its attention capabilities are limited by its tiny embedding dimension.
